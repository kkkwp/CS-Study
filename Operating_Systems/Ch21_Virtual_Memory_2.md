 목차
- 운영체제란?
- 컴퓨터시스템의 구조
- 프로세스 관리
- CPU 스케쥴링
- 병행 제어
- 데드락
- 메모리 관리
- **가상 메모리**
- 파일 시스템
- 입출력 시스템
- 디스크 관리

---

# **[ 가상메모리2 ]**

가상 메모리1 파트에서는 가상메모리 시스템에서 어떤 페이지가 쫓겨나야 할 때 쫓아낼 페이지를 결정하는 Replacement algorithm를 배웠고,

**Replacement algorithm은 가상 메모리 시스템에서만 사용하는 것이 아니고 캐슁 환경같은 다양한 환경에서도 사용한다.**

<aside>
💡 가상메모리 (페이징 시스템)

=> 빠른 공간 - 물리적(메인) 메모리

=> 느린 공간 - 디스크 (패킹 스토어, Swap area)

</aside>

# 캐싱 기법

빠른 속도를 보장하는 한정된 공간 (=캐시)에 요청된 데이터를 저장해 두었다가, 후속 요청 시 캐시로부터 직접 서비스하는 방식 (느린 저장 장치까지 가지 않고 캐시로 부터 받아서 사용 가능한 방식)

페이징 시스템 외에도 캐시 메모리, 버퍼 캐시, 웹 캐싱 등 다양한 분야에서 사용

- 캐시 메모리
    
    CPU와 메인 메모리 사이에는 캐시 메모리라는 빠른 계층이 존재한다. CPU에서 메모리에 접근할 때, 직접 접근하는 것이 아니고 메인 메모리에 접근하기 전에 요청한 내용이 캐시 메모리에 있는 지 먼저 확인 후 없는 경우에 메인 메모리에 요청을 보낸다.
    
    TLB(Translation Lookaside Buffer: 메인 메모리에 위치해있는 페이지 테이블에 매번 접근하는 것을 막고자 페이지 테이블의 일부 항목을 캐시로 저장하여 주소 변환을 빠르게 수행하기 위한 하드웨어 캐시)도 특유의 캐시 메모리이다. 
    
    주소 변환 뿐 아니라 일반적인 메모리 참조에서도 캐시메모리는 사용되고 있다.
    
- 버퍼 캐싱
    
    파일 시스템에 대한 read, write 요청을 메모리에서 빠르게 서비스하는 방식
    
    버퍼 캐싱과 페이징 시스템의 매체 (빠른 공간, 느린공간)는 동일하다.
    
    - paging system - Swap area
    - Buffer caching - File system
- 웹 캐싱
    
    웹 페이지에 대한 요청을 하면 웹 서버에서 해당 페이지를 가져와서 웹 브라우저에 표시를 해준다. 이 때 동일한 URL에 대해 잠시 후 다시 요청을 하면 멀리 있는 웹 서버에서 해당 내용을 읽어들이는데 시간이 걸리기 때문에 처음에 읽어온 웹 페이지를 저장해뒀다가 보여주는 방식이다.
    

> 캐시 메모리, 버퍼 캐싱
> 

단일 시스템에서 **저장매체 간 속도 차이**가 나서 저장하는 경우

> 웹 캐싱
> 

시스템이 다른 상황에서 **지리적으로 멀리 떨어져**있기 때문에 저장하는 경우

### 제약 조건

replacement algorithm에는 시간 제약 조건이 있기에 삭제 항목을 결정하는 일에 지나치게 많은 시간을 소비할 수 없다.

- O(n)의 시간 복잡도는 오버헤드가 크기 때문에 실제 시스템에서는 사용할 수 없으며
- **O(1),O(log n)까지 허용**
    - 캐싱 환경의 LRU
        
        <img src ="https://github.com/kkkwp/CS-Study/assets/113974911/10a04e10-515a-4dca-a4d5-eeb59adfa915">
        참조된 시간 순으로 줄세우기 진행
        
        - 새로운 페이지가 메모리에 들어오면 맨 아래에 추가 (아래에 있을 수록 최근에 참조된 것)
        - 메모리 안에서 어떤 페이지가 다시 참조되면 해당 노드를 빼서 가장 아래 부분으로 추가
        - 쫓아내야 하는 경우 → 가장 위에 있는 걸 쫓아냄
        
        ⇒ 비교할 필요 없이 항상 맨 위의 노드를 쫓아내기 때문에 시간 복잡도는 O(1)
        
    - 캐싱 환경의 LFU
        
        <img src ="https://github.com/kkkwp/CS-Study/assets/113974911/12ad670e-fded-4e78-87cc-470de82462cf">

        참조된 횟수로 줄세우기 진행
        
        - 위로 갈 수록 참조 횟수가 적고, 아래로 갈 수록 참조 횟수 많음
        - 어떤 페이지가 참조 되었을 시 참조 횟수에 1을 더하는데, 아래의 노드들과 **비교를 통해 위치를 찾아야 해서** 최악의 경우 O(n)의 시간 복잡도를 가지고 있음.
        - 쫓아내야 하는 경우 → 가장 위에 있는 걸 쫓아 냄
        
        ⇒ heap 데이터 구조를 사용해 시간복잡도 문제 해결 가능
        
        <img src ="https://github.com/kkkwp/CS-Study/assets/113974911/448611fc-a75d-4e74-be83-08ffda952c78">
        
        노드를 한 줄로 세우지 않고 트리 형태로 구성해 항상 부모보다 자식의 참조 횟수가 많도록 구성 (MinHeap)
        
        - heap을 재구성하는 오버헤드는 O(log n)
        - 참조 횟수가 1 늘어날 경우, 자식 중에 부모보다 작은 자식이 있다면 자리를 바꿈 (반복)
        - 쫓아낼 때는 root에 있는 페이지를 쫓아내면 됨
        
        ⇒ 다시 참조되었을 때 일일이 다 비교하지 않고 직계 자손 경로만 따라 내려가면 되기에 시간복잡도가 O(log n)으로 감소
        

캐싱 환경에서 LRU, LFU가 사용 가능했던 반면,

페이징 시스템의 경우는 사용할 수 없다.

왜일까?

# 페이징 시스템의 제약 조건

<img src ="https://github.com/kkkwp/CS-Study/assets/113974911/8dc0c515-dd9c-40ad-acf1-55d64f9e5b53">

LRU와 LFU는 페이지 교체 알고리즘으로서 추가적인 메타데이터 추적이 필요

- 주소 변환 시 해당 페이지가 물리 메모리에 있다면 ⇒  CPU가 OS에 넘어갈 필요가 없기 때문에 페이지 관련 메타 데이터(페이지 사용 빈도, 시간)를 OS가 알 수 없고
- 주소 변환 시 해당 페이지가 물리 메모리에 없어 Page Fault가 일어나고, OS가 CPU 제어권을 갖게 되어 자료구조의 조작이 가능 하더라도 ⇒ OS는 페이지가 메모리에 올라온 시간과 같은 반쪽짜리 정보만을 볼 수 있기에, 참조 시간이나 참조 빈도등을 필요로 하는 LRU나 LFU 알고리즘을 사용할 수가 없음.

그러면 가상메모리에서는 쫓아낼 페이지를 결정하기 위해 어떤 알고리즘을 사용할까?

# ⇒ Clock Algorithm

> LRU 알고리즘을 근사 시킨 알고리즘으로 Second chance algorithm, NUR, NRU라고도 부름
> 
- NRU (Not Recetntly Used)
    - 오래된 페이지는 알 수 없지만 최근에 사용되지 않은 페이지는 알 수 있기에 이 방식으로 페이지를 쫓아 냄
- Clock?
    - 동그랗게 그려두고 시계 바늘이 움직이는 것 처럼 알고리즘을 운영하는 형태라 클락 알고리즘이라고 부름
        
        ⇒ circular linked list 사용
        

<img src ="https://github.com/kkkwp/CS-Study/assets/113974911/e8ef0377-7c67-40d4-9211-465f261333dc">

> Reference bit
> 

이미 메모리에 있는 어떤 페이지가 주소변환 하드웨어에 의해 주소변환 된 후 CPU가 그 페이지를 사용하게 되면, 페이지에 있는 Reference bit를 1로 세팅

⇒ 하드웨어가 레퍼런스 비트를 세팅해 페이지가 참조되었음을 알림

Page fault가 발생해 페이지를 쫓아내야할 경우 하드웨어가 세팅해둔 레퍼런스 비트를 참조

- 참조 비트가 1인 경우 : 최근에 한 번 참조가 되었구나라고 확인하고 해당 비트를 0으로 변경
- 참조 비트가 g0인 경우 : 해당 페이지를 쫓아냄
- 운영 방식
    1. OS가 시계 방향으로 돌며 참조 비트가 1이면 참조를 확인하고 해당 비트를 0으로 변경하면서 지나간다.
    2. 참조 비트가 0이라는 건 시계가 한바퀴 돌 동안에 이 페이지에 대한 참조가 없었다는 것이기 때문에 해당 페이지를 쫓아낸다.
    3. 참조 비트가 1이라는 건 도는 동안에 적어도 1번의 참조가 있음을 인지하고 넘어간다.
    
    ⇒ 참조 비트가 0인걸 쫓아내기에 가장 오래된 페이지를 쫓아내는 건 아니다.
    

> Modified bit
> 

메모리의 페이지가 참조될 때 read로도 참조될 수 있지만 write로도 참조 될 수 있는데, 이 때  modified bit를 하드웨어가 1로 변경

- 어떤 페이지가 쫓겨날 때 modified bit이 0이라고 하면 이 페이지가 패킹 스토어에서 물리 메모리로 올라온 이후 write가 발생하지 않은 것으로 볼 수 있어, 이런 페이지는 패킹 스토어에 이미 동일한 카피가 존재하기에 그냥 쫓아낸다.
- 어떤 페이지가 쫓겨날 때 modified bit이 1이라고 한다면 메모리에 올라오고 나서 적어도 한번은 CPU에서 write가 발생한 것으로 보고, 해당 페이지를 쫓아낼 땐 패킹 스토어에 수정된 내용을 반영하고 나서 지워야한다.

<aside>
💡 Clock 알고리즘을 더 개선해서 사용하기 위해서 Modified bit가 1인걸 쫓아내지 않고 0인걸 우선해서 쫓아낸다면 디스크에 쫓아낸 페이지에 대한 변경된 내용을 써줄 필요가 없기 때문에 조금 더 빠르게 사용 가능하다.

</aside>

reference bit만 사용 - Second chance algorithm(Clock algorithm)

reference, modify bit 같이 사용 - 개선된 Second chance algorithm(NUR)

# Page Frame의 Allocation

기존에 살펴본 알고리즘에서는 프로그램 여러개가 물리적 메모리에 같이 올라가있을 때, 어떤 페이지를 쫓아내는 경우 그 페이지가 무슨 프로세스에 속한 페이지인지와는 무관하게 쫓아낸다. 실제로 프로그램이 원활하게 실행되기 위해서는 CPU에서 실행되는 도중 page fault가 일어나지 않는 방향으로 구현되어야 한다.

그러기 위해서는 일련의 페이지들이 메모리에 같이 올라와 있어야 효율적일 것이다.

**Allocation Example**

Instrucation를 실행하는데 있어서 어떤 for문을 돌고 있다고 가정해보자. for문을 구성하는 페이지는 3개다.

**. 이 프로그램에 3개의 페이지를 주면 for문을 도는 동안에 page fault는 발생하지 않는다.**

for문을 백만번 돈다고 해서 for문을 도는 동안에 page fault가 한 번도 발생하지 않는다. 왜냐면 for문을 구성하는 페이지와 주어진 페이지가 동일하기 때문이다.

프로그램 별로 최소한의 페이지를 줘야지 page fault가 발생하지 않는다.

예를 들어서 프로그램이 실행되면서 명령어만 실행되는 게 아니라 데이터를 접근해야하는 경우도 있다고 한다면, 코드에 해당하는 페이지말고 데이터에 해당하는 페이지도 메모리에 같이 올라와 있어야만 page fault가 덜 난다.

프로그램별로 페이지 할당을 해주지 않으면 메모리에서 특정 프로그램이 페이지 프레임을 장악하는 일이 발생할 수 있다.

이 문제는 시스템의 비효율을 발생시킬 수 있기에 각가 프로그램에게 어느정도의 메모리 페이지를 나누어 주는 Allocation 방식이 등장했다.

1. **Equal allocation** (균등): 모든 프로세스에게 똑같은 개수의 메모리 페이지 할당
    - 어떤 프로그램은 페이지를 많이 필요로 하고 어떤 페이지는 적게 필요로 하기 때문에 비효율적일 수 있음.
2. **Proportional allocation** (크기 비례): 크기에 비례해서 메모리 페이지 할당
    - 같은 프로그램이라고 하더라도 시간에 따라서 필요한 페이지 크기가 다를 수 있음.
3. **Priority allocation** (우선순위 비례): CPU 우선순위가 높은 것에다가 메모리 페이지를 더 많이 할당

⇒ 프로그램마다 필요한 페이지를 미리 할당하고 page fault를 줄일 수 있게 됐다!

(사실은 할당하지 않더라도 LRU, LFU같은 replacement algorithm를 사용하다보면 알아서 어느정도 할당되는 효과가 발생하기도 함)

LRU, LFU같은 방식은 페이지 교체 알고리즘일 뿐 프로그램에게 페이지를 할당하는 효과는 없다.

 그러나 Global replacement의 방식에서 사용하는 Working set, PFF는 **프로그램이 최소로 필요로 하는 페이지를 동시에 메모리에 올려놓는 할당 효과가 있는 알고리즘이다**.

# Global & Local Replacement

미리 할당하는 방법을 사용하지 않고 프로세스 별로 원하는 때마다 메모리 할당량을 자동으로 조절해서 사용하는 방식

> **Global replacement (다른 프로세스와 페이지 할당 경쟁)**
> 
> - replace 시 다른 프로세스에 할당된 프레임을 빼앗아 올 수 있음 (= Replace 시 다른 프로세스의 페이지 쫓아내기 가능)
> - Working set, PFF 알고리즘 사용 (미리 할당하지 않더라도 알고리즘이 마치 할당을 한 것 같은 효과를 내면서 운영이 된다.)
> - FIFO, LRU, LFU 등의 알고리즘을 global replacement로 사용 가능

> **Local replacement (주어진 페이지 할당량 안에서 프로세스 별로 관리)**
> 
> - 새로운 페이지를 올려두기 위해 자신에게 할당된 페이지를 쫓아내는 방식(= 자신에게 할당된 프레임 내에서만 Replace 가능)
> - FIFO, LRU, LFU 등의 알고리즘을 프로세스 별로 운영

# Thrashing

프로그램에게 메모리가 너무 적게 할당되어 페이지 부재가 전체적으로 지나치게 많이 일어나는 현상

프로세스의 원활한 수행에 필요한 최소한의 페이지 프레임 수를 할당받지 못한 경우 발생

<img src ="https://github.com/kkkwp/CS-Study/assets/113974911/84564a2a-8415-4acd-aa39-01567398b672">

| 시점 | CPU 이용률 | 이유 |
| --- | --- | --- |
| 초반 | 낮다 | 처음에는 프로그램 하나만 메모리에 올라가 있기 때문에 CPU 이용률이 낮다. 해당 프로세스가 메모리만 쓰는 게 아니고 I/O가 일어나면 I/O를 하러가기 때문에 그 사이에 CPU가 놀게 된다. |
| 중반 | 높다 | 메모리에 프로그램을 더 올리면 하나가 I/O를 하러 간 사이에 다른 프로그램을 돌리기 때문에 놀지 않고 일하는 시간이 증가한다. 
ex) 10개의 프로그램 중 9개가 I/O를 하러 가더라도 레디큐에 당장 CPU만 주면 실행이 가능한 프로그램이 있기 때문에 MPD를 늘려주면 CPU 이용률이 올라간다. |
| 후반 | 낮다 | 이용률이 뚝 떨어지고 Thrashing 발생!!! |

프로세스한테 페이지를 너무 적게 할당하면

- Page fault rate 커짐
- CPU Utilization 낮아짐 (어떤 프로그램이 cpu를 잡더라도 page fault가 발생하기 때문)
    
    ⇒ 운영체제는 cpu가 놀고 있다고 판단, 또 다른 프로세스를 시스템에 추가 (multi programing을 늘림) 함 ⇒ 프로세스 당 할당된 페이지 프레임 수는 더욱 감소하여 page fault가 더욱 자주 발생하게 됨 ⇒ 프로세스는 스와핑 작업으로 매우 바쁘고 CPU는 할일이 없어서 한가한 상황에 처함
    

⇒ 매우 비효율적인 시스템이 됨

이렇듯 쓰레싱으로 인한 비효율을 막기 위해서는 

MPD(CPU에 올라가 있는 프로세스의 수)를 조절해줌으로써 프로그램들이 어느정도 메모리 확보가 가능하게 해줘야한다. 이 역할을 해주는 것이 Working-set, PFF 알고리즘이다.

### Working-set Model

특정 순간에 메모리에 꼭 올라와 있어야만 하는 (빈번히 참조되는 등) 페이지 집합 (=**워킹셋**)이 그 순간 메모리에 올라와 있는 걸 보장해주는 기법.

**Locality of reference**

프로그램이 특정 시간에 특정 메모리 위치만 참조하는 특징을 가지고 있음

> Example: for문이 실행된다고 하면 루프를 도는 동안에는 그 루프를 구성하는 페이지만 집중적으로 참조가 된다.
> 

워킹셋은 메모리에 한꺼번에 올라와 있도록 보장해주는 기법이 있어야지만 page fault가 나지 않는다.

<aside>
💡 워킹 셋은 페이지 5개로 구성되는데 메모리는 3개밖에 보장해줄 수 없는 상황이 발생한다면? ⇒ 모든 페이지를 통째로 반납. 5개를 위한 자리를 줄 때까지 하나도 받지 않는다. 전체를 다 반납하고 해당 프로세스는 디스크로 swap out! 그리고 프로세스는 suspended 상태로 들어감.

</aside>

Working set모델은 MPD를 조절해서 워킹 셋이 한꺼번에 메모리에 올라가는 것이 보장이 안된다면 그 프로세스의 메모리를 통째로 빼앗는 방식으로 thrashing을 방지하고 MPD를 조절한다.

알고리즘 동작 과정

: 우린 워킹셋을 미리 알 수 없다.  메모리에 올라가 있으면 좋은 페이지 집합을 알면 그걸 올려다두면 되겠지만 정확하게 모르기 때문에 과거를 통해서 워킹셋을 추정한다.

⇒ 프로그램이 과거 델타시간동안 참조한 페이지를 워킹셋으로 간주해서 과거 델타시간동안 참조된 페이지는 메모리에서 쫓아내지 않고 유지

<img src ="https://github.com/kkkwp/CS-Study/assets/113974911/39104765-ae0b-43c4-8143-dd6e8e83b082">

t1 = 현재시간

델타시간(윈도우 사이즈) = 10

t1을 기준으로 최근 참조된 10개의 페이지를 보고 그것들을 해당 프로그램의 워킹셋으로 간주 ⇒ 워킹셋 알고리즘은 5개의 페이지 프레임을 해당 프로그램에게 주어야 함 (만약 메모리가 부족해서 못주게 되면 전부 swap out 시키고 suspended 상태로 변경)

델타시간 동안 유지하다가 시간이 지나면 버리는 거라고 생각하면 됨 다시 참조되면 다시 델타 시간을 유지하다가 시간 지나면 버림

### PFF

MPD를 조절하면서 Thrashing를 방지하는 방법

워킹 셋처럼 추정하는 방식이 아니고 직접 page fault가 있는 지 보는 방식

현재 시점에 시스템에서 페이지 부재가 얼마나 일어나는 지, 특정 프로그램이 페이지 부재를 얼마나 내보내는 지 확인하는 방식

**프로그램**: page fault!!! page fault!!! page fault!!! page fault!!! page fault!!!


**PFF**: 저 프로그램의 워킹 셋이 메모리에 다 보장이 안되어 있는 상태구나? 페이지를 더 줘야 겠다.

- 프로그램에 할당되는 메모리 크기가 커지게 되면 page fault 발생 비율을 줄어듦
- page fault가 일정 수준 이상으로 발생하고 있다고 하면 워킹 셋이 보장이 안된 것으로 간주해서 프로그램에게 페이지 수를 늘려줌. page fault는 줄고 발생 빈도가 어느 정도 이내로 들어오게 됨.
- 어떤 프로그램이 page fault를 너무 발생시키지 않으면 페이지가 메모리를 많이 가지고 있다고 간주해서 메모리를 빼앗아서 일정수준의 page fault를 유지하게 함
- 만약 page fault 비율이 빈번하고 메모리를 더 줘야 하는데 줄 메모리가 없다면 그 프로그램을 통째로 Swap out 시켜서 메모리에 남아있는 프로그램이라도 page fault를 줄여서 thrashing를 방지한다.

### 페이지 사이즈 결정

> **페이징 시스템**
> 

: 동일한 크기의 페이지 단위로 물리적인 메모리의 프레임을 자르고 가상 메모리의 프로그램을 구성하는 페이지 주소 공간도 페이지 단위로 자른다.

- 페이지 사이즈는 보통 4KB를 사용하는데 메모리 공간이 32bit 주소 체계에서 64bit로 크기가 커지면서 페이지 크기도 커져야 한다.
- 페이지 크기가 너무 작으면 페이지 테이블이 커져야 한다.

페이지 사이즈를 줄이면

똑같은 크기의 메모리에서 더 잘게 썰기 때문에 페이지 개수가 증가하게 되고, 테이블 엔트리 수가 더 많이 필요하기 때문에 페이지 테이블을 위한 메모리 낭비가 심해지게 된다.

⇒ 페이지 안에서 사용이 안되는 부분이 생기기 마련인데, 페이지 사이즈를 그 문제는 줄어든다. 잘게 나누기 때문에 특히 꼭 필요한 정보만 메모리에 올릴 수 있으므로 메모리 이용이 더 효율적이다.

페이지 크기가 크면

페이지 안에서 아주 작은 부분만 필요한데 page fault가 발생했을 때 큰 페이지를 통째로 메모리에 올려야하기 때문에 꼭 필요한 정보 외에 것들이 올라갈 수 있다

⇒ locality 활용 측면에서는 페이지 크기가 큰게 좋다. locality를 가진 프로그램에서, 함수가 실행되면 그 함수를 구성하는 코드들은 순차적으로 참조가 된다. 이 때 page fault가 발생했을 때 페이지 하나를 통째로 올려놓으면 아래쪽에 있는 메모리 위치들은 메모리에 올라온 이후 page fault를 발생시키지 않고 참조하기 때문에 더 효율적인 측면이 있다.

> disk transfer의 효율성
> 

 = 페이지가 클수록 좋다

디스크라는 건 seek를 해야한다. 디스크 원판이 회전하면서 디스크 헤드가 이동을 하고 특정 위치 섹터를 읽거나 써야 하는데 사실 seek하는 시간이 대단히 오래 걸린다. 따라서 가능하면 한 번 디스크 헤드가 이동해서 많은 양의 뭉치를 읽어서 메모리에 올리는 것이 좋기 때문에 페이지 크기가 큰 게 좋다. 작으면 계속 page fault가 발생하고 디스크 헤드가 움직여야 하는 seek 시간이 길어져서 비효율적이다

요즘은 페이지 크기를 점점 더 키워주는 추세